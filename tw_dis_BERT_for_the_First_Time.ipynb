{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"tw_dis_BERT_for_the_First_Time.ipynb","provenance":[],"collapsed_sections":[],"toc_visible":true,"machine_shape":"hm"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.4"}},"cells":[{"cell_type":"markdown","metadata":{"colab_type":"text","id":"view-in-github"},"source":["<a href=\"https://colab.research.google.com/github/jalammar/jalammar.github.io/blob/master/notebooks/bert/A_Visual_Notebook_to_Using_BERT_for_the_First_Time.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"]},{"cell_type":"code","metadata":{"id":"tgCAmGYCLhFV","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":326},"outputId":"3712e765-cd2c-4152-a459-4cb31ff02403","executionInfo":{"status":"ok","timestamp":1589896743935,"user_tz":-180,"elapsed":4653,"user":{"displayName":"Nadya Strogankova","photoUrl":"","userId":"11793698916100822526"}}},"source":["\n","!pip install transformers\n"],"execution_count":30,"outputs":[{"output_type":"stream","text":["Requirement already satisfied: transformers in /usr/local/lib/python3.6/dist-packages (2.9.1)\n","Requirement already satisfied: tokenizers==0.7.0 in /usr/local/lib/python3.6/dist-packages (from transformers) (0.7.0)\n","Requirement already satisfied: sacremoses in /usr/local/lib/python3.6/dist-packages (from transformers) (0.0.43)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers) (3.0.12)\n","Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers) (2.23.0)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers) (2019.12.20)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers) (4.41.1)\n","Requirement already satisfied: sentencepiece in /usr/local/lib/python3.6/dist-packages (from transformers) (0.1.90)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers) (1.18.4)\n","Requirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from transformers) (0.7)\n","Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (7.1.2)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (0.14.1)\n","Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (1.12.0)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (1.24.3)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (3.0.4)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2.9)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2020.4.5.1)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab_type":"code","id":"fvFvBLJV0Dkv","colab":{}},"source":["import numpy as np\n","import pandas as pd\n","from sklearn.model_selection import train_test_split\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.model_selection import GridSearchCV\n","from sklearn.model_selection import cross_val_score\n","import torch\n","import transformers as ppb\n","import warnings\n","warnings.filterwarnings('ignore')"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"zQ-42fh0hjsF"},"source":["## Importing the dataset\n","We'll use pandas to read the dataset and load it into a dataframe."]},{"cell_type":"code","metadata":{"colab_type":"code","id":"cyoj29J24hPX","colab":{"base_uri":"https://localhost:8080/","height":34},"outputId":"604de186-8831-4cbc-bf27-f0601c9bca46","executionInfo":{"status":"ok","timestamp":1589896743937,"user_tz":-180,"elapsed":4614,"user":{"displayName":"Nadya Strogankova","photoUrl":"","userId":"11793698916100822526"}}},"source":["from google.colab import drive\n","drive.mount('/content/drive')\n"],"execution_count":32,"outputs":[{"output_type":"stream","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"wW4q7beaNtoG","colab_type":"code","colab":{}},"source":["#with open('/content/drive/My Drive/Colab Notebooks/data_processed.csv', 'r') as f:\n","#  f.open()\n","#df = pd.read_csv('data_processed.csv')\n","df = pd.read_csv(\"/content/drive/My Drive/Colab Notebooks/data_processed.csv\", sep=\"#\")"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"wpubwN_sb_eX","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"outputId":"ccd40a74-9797-40dd-ac8e-5270b3038baf","executionInfo":{"status":"ok","timestamp":1589896743938,"user_tz":-180,"elapsed":4591,"user":{"displayName":"Nadya Strogankova","photoUrl":"","userId":"11793698916100822526"}}},"source":["df.shape"],"execution_count":34,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(10876, 9)"]},"metadata":{"tags":[]},"execution_count":34}]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"7_MO08_KiAOb"},"source":["## Loading the Pre-trained BERT model\n","Let's now load a pre-trained BERT model. "]},{"cell_type":"code","metadata":{"colab_type":"code","id":"q1InADgf5xm2","colab":{}},"source":["# For DistilBERT:\n","model_class, tokenizer_class, pretrained_weights = (ppb.DistilBertModel, ppb.DistilBertTokenizer, 'distilbert-base-uncased')\n","\n","## Want BERT instead of distilBERT? Uncomment the following line:\n","#model_class, tokenizer_class, pretrained_weights = (ppb.BertModel, ppb.BertTokenizer, 'bert-base-uncased')\n","\n","# Load pretrained model/tokenizer\n","tokenizer = tokenizer_class.from_pretrained(pretrained_weights)\n","model = model_class.from_pretrained(pretrained_weights)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"lZDBMn3wiSX6"},"source":["Right now, the variable `model` holds a pretrained distilBERT model -- a version of BERT that is smaller, but much faster and requiring a lot less memory.\n","\n","## Model #1: Preparing the Dataset\n","Before we can hand our sentences to BERT, we need to so some minimal processing to put them in the format it requires.\n","\n","### Tokenization\n","Our first step is to tokenize the sentences -- break them up into word and subwords in the format BERT is comfortable with."]},{"cell_type":"code","metadata":{"id":"mC_zkFUlCYar","colab_type":"code","colab":{}},"source":["tags_simple = pd.read_csv(\"/content/drive/My Drive/Colab Notebooks/tags_simple.csv\")\n","#print(tags_simple.isna())\n","tags_simple.fillna(\"\", inplace=True)\n","#print(tags_simple.head())\n","tags_tokenized = tags_simple[\"tags\"].apply((lambda x: tokenizer.encode(x, add_special_tokens=True)))"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"QLzif8sMa-Ka","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":241},"outputId":"80e80a5d-f2f3-4077-cf29-390b51a49e2e","executionInfo":{"status":"ok","timestamp":1589900116827,"user_tz":-180,"elapsed":7719,"user":{"displayName":"Nadya Strogankova","photoUrl":"","userId":"11793698916100822526"}}},"source":["!pip install emoji\n","print(tokenizer.tokenize('I ❤️ you'))\n","s = u'\\U0001f600'\n","from emoji.unicode_codes import UNICODE_EMOJI\n","\n","print(UNICODE_EMOJI[s])"],"execution_count":86,"outputs":[{"output_type":"stream","text":["Collecting emoji\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/40/8d/521be7f0091fe0f2ae690cc044faf43e3445e0ff33c574eae752dd7e39fa/emoji-0.5.4.tar.gz (43kB)\n","\r\u001b[K     |███████▌                        | 10kB 19.2MB/s eta 0:00:01\r\u001b[K     |███████████████                 | 20kB 2.2MB/s eta 0:00:01\r\u001b[K     |██████████████████████▋         | 30kB 2.8MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▏ | 40kB 3.1MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 51kB 2.0MB/s \n","\u001b[?25hBuilding wheels for collected packages: emoji\n","  Building wheel for emoji (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for emoji: filename=emoji-0.5.4-cp36-none-any.whl size=42176 sha256=df7e24d81e7af2afa2794a93b39560974906bfbb2c1778e85294fed751280293\n","  Stored in directory: /root/.cache/pip/wheels/2a/a9/0a/4f8e8cce8074232aba240caca3fade315bb49fac68808d1a9c\n","Successfully built emoji\n","Installing collected packages: emoji\n","Successfully installed emoji-0.5.4\n","['i', '[UNK]', 'you']\n",":grinning_face:\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab_type":"code","id":"Dg82ndBA5xlN","colab":{}},"source":["tokenized = df[\"text\"].apply((lambda x: tokenizer.encode(x, add_special_tokens=True)))"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"mHwjUwYgi-uL"},"source":["<img src=\"https://jalammar.github.io/images/distilBERT/bert-distilbert-tokenization-2-token-ids.png\" />\n","\n","### Padding\n","After tokenization, `tokenized` is a list of sentences -- each sentences is represented as a list of tokens. We want BERT to process our examples all at once (as one batch). It's just faster that way. For that reason, we need to pad all lists to the same size, so we can represent the input as one 2-d array, rather than a list of lists (of different lengths)."]},{"cell_type":"code","metadata":{"colab_type":"code","id":"URn-DWJt5xhP","colab":{}},"source":["max_len = 0\n","for i in tokenized.values:\n","    if len(i) > max_len:\n","        max_len = len(i)\n","\n","padded = np.array([i + [0]*(max_len-len(i)) for i in tokenized.values])"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"aU9jTdNTDbsN","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"xI7SFepeDk3L","colab":{}},"source":["max_len = 0\n","for i in tags_tokenized.values:\n","    if len(i) > max_len:\n","        max_len = len(i)\n","\n","tags_padded = np.array([i + [0]*(max_len-len(i)) for i in tags_tokenized.values])"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"Mdjg306wjjmL"},"source":["Our dataset is now in the `padded` variable, we can view its dimensions below:"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"jdi7uXo95xeq","outputId":"c39b032c-9f1b-43d8-b3fe-66d011afbb24","colab":{"base_uri":"https://localhost:8080/","height":34},"executionInfo":{"status":"ok","timestamp":1589896820989,"user_tz":-180,"elapsed":6261,"user":{"displayName":"Nadya Strogankova","photoUrl":"","userId":"11793698916100822526"}}},"source":["np.array(tags_padded).shape"],"execution_count":40,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(10876, 34)"]},"metadata":{"tags":[]},"execution_count":40}]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"sDZBsYSDjzDV"},"source":["### Masking\n","If we directly send `padded` to BERT, that would slightly confuse it. We need to create another variable to tell it to ignore (mask) the padding we've added when it's processing its input. That's what attention_mask is:"]},{"cell_type":"markdown","metadata":{"id":"beQDugOW-k0t","colab_type":"text"},"source":[""]},{"cell_type":"code","metadata":{"colab_type":"code","id":"4K_iGRNa_Ozc","outputId":"30747e79-73a5-4095-d955-ebdbac5d39ea","colab":{"base_uri":"https://localhost:8080/","height":34},"executionInfo":{"status":"ok","timestamp":1589896820990,"user_tz":-180,"elapsed":6248,"user":{"displayName":"Nadya Strogankova","photoUrl":"","userId":"11793698916100822526"}}},"source":["attention_mask = np.where(padded != 0, 1, 0)\n","attention_mask.shape"],"execution_count":41,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(10876, 54)"]},"metadata":{"tags":[]},"execution_count":41}]},{"cell_type":"code","metadata":{"colab_type":"code","outputId":"fd21d1b6-b1b2-4acd-dd96-279fcdcdf575","executionInfo":{"status":"ok","timestamp":1589896820990,"user_tz":-180,"elapsed":6237,"user":{"displayName":"Nadya Strogankova","photoUrl":"","userId":"11793698916100822526"}},"id":"Kb8ctzydDzy0","colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["tags_attention_mask = np.where(tags_padded != 0, 1, 0)\n","tags_attention_mask.shape"],"execution_count":42,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(10876, 34)"]},"metadata":{"tags":[]},"execution_count":42}]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"jK-CQB9-kN99"},"source":["## Model #1: And Now, Deep Learning!\n","Now that we have our model and inputs ready, let's run our model!\n","\n","<img src=\"https://jalammar.github.io/images/distilBERT/bert-distilbert-tutorial-sentence-embedding.png\" />\n","\n","The `model()` function runs our sentences through BERT. The results of the processing will be returned into `last_hidden_states`."]},{"cell_type":"code","metadata":{"colab_type":"code","id":"39UVjAV56PJz","colab":{}},"source":["input_ids = torch.tensor(padded)  \n","attention_mask = torch.tensor(attention_mask)\n","\n","with torch.no_grad():\n","    last_hidden_states = model(input_ids, attention_mask=attention_mask)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"5v_W6SQcD6X9","colab":{}},"source":["tags_input_ids = torch.tensor(tags_padded)  \n","tags_attention_mask = torch.tensor(tags_attention_mask)\n","\n","with torch.no_grad():\n","    tags_last_hidden_states = model(tags_input_ids, attention_mask=tags_attention_mask)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"FoCep_WVuB3v"},"source":["Let's slice only the part of the output that we need. That is the output corresponding the first token of each sentence. The way BERT does sentence classification, is that it adds a token called `[CLS]` (for classification) at the beginning of every sentence. The output corresponding to that token can be thought of as an embedding for the entire sentence.\n","\n","<img src=\"https://jalammar.github.io/images/distilBERT/bert-output-tensor-selection.png\" />\n","\n","We'll save those in the `features` variable, as they'll serve as the features to our logitics regression model."]},{"cell_type":"code","metadata":{"colab_type":"code","id":"C9t60At16PVs","colab":{}},"source":["features = last_hidden_states[0][:,0,:].numpy()\n","tags_features = tags_last_hidden_states[0][:,0,:].numpy()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"ySOreuVF_G2L","colab_type":"code","colab":{}},"source":["pd.DataFrame(features).to_csv('/content/drive/My Drive/Colab Notebooks/features.csv')\n","pd.DataFrame(tags_features).to_csv('/content/drive/My Drive/Colab Notebooks/tags_features.csv')"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"VqL4xbgonUot","colab_type":"code","colab":{}},"source":["#cat = pd.read_csv(\"/content/drive/My Drive/Colab Notebooks/cat.csv\")\n","#tags = pd.read_csv(\"/content/drive/My Drive/Colab Notebooks/tags.csv\")\n","# добавление тегов в виде разреженной матрицы изрядно уменьшило результат. \n","#ff = np.hstack((features, tags))"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"IPbHaFroE09j","colab_type":"code","colab":{}},"source":["ff = np.hstack((features, tags_features))"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"_VZVU66Gurr-"},"source":["The labels indicating which sentence is positive and negative now go into the `labels` variable"]},{"cell_type":"code","metadata":{"id":"Acj-hwZqRBw4","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"outputId":"24b27a17-060c-4dd5-baf0-13a75066b726","executionInfo":{"status":"ok","timestamp":1589897605304,"user_tz":-180,"elapsed":790488,"user":{"displayName":"Nadya Strogankova","photoUrl":"","userId":"11793698916100822526"}}},"source":["ff.shape"],"execution_count":49,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(10876, 1536)"]},"metadata":{"tags":[]},"execution_count":49}]},{"cell_type":"code","metadata":{"colab_type":"code","id":"JD3fX2yh6PTx","colab":{}},"source":["labels = df[\"target\"]"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"if32g5UZnAxI","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"outputId":"caa7ed7a-7012-4349-f45a-73aa8126f446","executionInfo":{"status":"ok","timestamp":1589898784630,"user_tz":-180,"elapsed":842,"user":{"displayName":"Nadya Strogankova","photoUrl":"","userId":"11793698916100822526"}}},"source":["#с учетом тегов\n","#train_f = ff[:7613]\n","#test_f = ff[7613:]\n","# без учета тегов\n","train_f = features[:7613]\n","test_f = features[7613:]\n","\n","train_l = labels[:7613]\n","test_l = labels[7613:]\n","\n","print(len(train_l))"],"execution_count":73,"outputs":[{"output_type":"stream","text":["7613\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"iaoEvM2evRx1"},"source":["## Model #2: Train/Test Split\n","Let's now split our datset into a training set and testing set (even though we're using 2,000 sentences from the SST2 training set)."]},{"cell_type":"code","metadata":{"colab_type":"code","id":"ddAqbkoU6PP9","colab":{}},"source":["train_features, test_features, train_labels, test_labels = train_test_split(train_f, train_l, shuffle=\"False\")"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"B9bhSJpcv1Bl"},"source":["<img src=\"https://jalammar.github.io/images/distilBERT/bert-distilbert-train-test-split-sentence-embedding.png\" />\n","\n","### [Bonus] Grid Search for Parameters\n","We can dive into Logistic regression directly with the Scikit Learn default parameters, but sometimes it's worth searching for the best value of the C parameter, which determines regularization strength."]},{"cell_type":"code","metadata":{"colab_type":"code","id":"cyEwr7yYD3Ci","colab":{"base_uri":"https://localhost:8080/","height":51},"outputId":"47d29b31-0bf6-45fa-a387-c0de6eb3ae0d","executionInfo":{"status":"ok","timestamp":1589898820482,"user_tz":-180,"elapsed":36674,"user":{"displayName":"Nadya Strogankova","photoUrl":"","userId":"11793698916100822526"}}},"source":["parameters = {'C': np.linspace(0.0001, 100, 20)}\n","grid_search = GridSearchCV(LogisticRegression(), parameters)\n","grid_search.fit(train_features, train_labels)\n","\n","print('best parameters: ', grid_search.best_params_)\n","print('best scrores: ', grid_search.best_score_)"],"execution_count":75,"outputs":[{"output_type":"stream","text":["best parameters:  {'C': 5.263252631578947}\n","best scrores:  0.8008397402346239\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"KCT9u8vAwnID"},"source":["We now train the LogisticRegression model. If you've chosen to do the gridsearch, you can plug the value of C into the model declaration (e.g. `LogisticRegression(C=5.2)`)."]},{"cell_type":"code","metadata":{"colab_type":"code","id":"gG-EVWx4CzBc","outputId":"ac6cd427-9238-4b23-aaf4-21e4b30661ac","colab":{"base_uri":"https://localhost:8080/","height":102},"executionInfo":{"status":"ok","timestamp":1589898820767,"user_tz":-180,"elapsed":36949,"user":{"displayName":"Nadya Strogankova","photoUrl":"","userId":"11793698916100822526"}}},"source":["lr_clf = LogisticRegression( C = grid_search.best_params_['C'])\n","lr_clf.fit(train_features, train_labels)"],"execution_count":76,"outputs":[{"output_type":"execute_result","data":{"text/plain":["LogisticRegression(C=5.263252631578947, class_weight=None, dual=False,\n","                   fit_intercept=True, intercept_scaling=1, l1_ratio=None,\n","                   max_iter=100, multi_class='auto', n_jobs=None, penalty='l2',\n","                   random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n","                   warm_start=False)"]},"metadata":{"tags":[]},"execution_count":76}]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"3rUMKuVgwzkY"},"source":["<img src=\"https://jalammar.github.io/images/distilBERT/bert-training-logistic-regression.png\" />\n","\n","## Evaluating Model #2\n","So how well does our model do in classifying sentences? One way is to check the accuracy against the testing dataset:"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"iCoyxRJ7ECTA","outputId":"84197577-7a46-4e01-87bd-9b3e854e9d53","colab":{"base_uri":"https://localhost:8080/","height":34},"executionInfo":{"status":"ok","timestamp":1589898820768,"user_tz":-180,"elapsed":36938,"user":{"displayName":"Nadya Strogankova","photoUrl":"","userId":"11793698916100822526"}}},"source":["lr_clf.score(test_features, test_labels)"],"execution_count":77,"outputs":[{"output_type":"execute_result","data":{"text/plain":["0.8182773109243697"]},"metadata":{"tags":[]},"execution_count":77}]},{"cell_type":"code","metadata":{"id":"YpERCq-YrGuW","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"outputId":"c2a15ba8-e3b3-4a30-f2cb-bc99728fef0d","executionInfo":{"status":"ok","timestamp":1589898820769,"user_tz":-180,"elapsed":36929,"user":{"displayName":"Nadya Strogankova","photoUrl":"","userId":"11793698916100822526"}}},"source":["Y_pred = lr_clf.predict(test_f)\n","len(Y_pred)"],"execution_count":78,"outputs":[{"output_type":"execute_result","data":{"text/plain":["3263"]},"metadata":{"tags":[]},"execution_count":78}]},{"cell_type":"code","metadata":{"id":"E8qw5FRmr2Iw","colab_type":"code","colab":{}},"source":["import time\n","submission = pd.DataFrame({\"id\": df[\"id\"][7613:], \"target\": Y_pred }, dtype = \"int\")\n","fname = '/content/drive/My Drive/Colab Notebooks/' + time.ctime() + 'submission.csv'\n","submission.to_csv(fname, index=False, header=\"id,target\")"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"75oyhr3VxHoE"},"source":["How good is this score? What can we compare it against? Let's first look at a dummy classifier:"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"lnwgmqNG7i5l","outputId":"b0331c44-a448-44ca-a214-f14a0deac397","colab":{"base_uri":"https://localhost:8080/","height":34},"executionInfo":{"status":"ok","timestamp":1589898822096,"user_tz":-180,"elapsed":38242,"user":{"displayName":"Nadya Strogankova","photoUrl":"","userId":"11793698916100822526"}}},"source":["from sklearn.naive_bayes import BernoulliNB\n","clf = BernoulliNB()\n","\n","scores = cross_val_score(clf, train_features, train_labels)\n","print(\"BNB classifier score: %0.3f (+/- %0.2f)\" % (scores.mean(), scores.std() * 2))"],"execution_count":80,"outputs":[{"output_type":"stream","text":["BNB classifier score: 0.762 (+/- 0.02)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"6lbRqInYVet4","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"outputId":"8ddcbef8-d5af-45f7-ad9c-c54890e5144a","executionInfo":{"status":"ok","timestamp":1589898822097,"user_tz":-180,"elapsed":38229,"user":{"displayName":"Nadya Strogankova","photoUrl":"","userId":"11793698916100822526"}}},"source":["np.array(test_labels).reshape(-1, 1).shape"],"execution_count":81,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(1904, 1)"]},"metadata":{"tags":[]},"execution_count":81}]},{"cell_type":"code","metadata":{"colab_type":"code","outputId":"11ddfa91-5b34-4fa0-c55c-02ca6c0019f4","executionInfo":{"status":"ok","timestamp":1589898825145,"user_tz":-180,"elapsed":41263,"user":{"displayName":"Nadya Strogankova","photoUrl":"","userId":"11793698916100822526"}},"id":"hhqoUprVTUE_","colab":{"base_uri":"https://localhost:8080/","height":51}},"source":["from sklearn.svm import LinearSVC\n","clf = LinearSVC( dual=False, tol=1e-3)\n","clf.fit(train_features, np.array(train_labels).reshape(-1, 1))\n","print(test_features.shape, test_labels.shape)\n","scores = clf.score(test_features, np.array(test_labels).reshape(-1, 1))\n","print(\"LinearSVC classifier score: %0.3f (+/- %0.2f)\" % (scores.mean(), scores.std() * 2))"],"execution_count":82,"outputs":[{"output_type":"stream","text":["(1904, 768) (1904,)\n","LinearSVC classifier score: 0.810 (+/- 0.00)\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"7Lg4LOpoxSOR"},"source":["So our model clearly does better than a dummy classifier. But how does it compare against the best models?\n","\n","## Proper SST2 scores\n","For reference, the [highest accuracy score](http://nlpprogress.com/english/sentiment_analysis.html) for this dataset is currently **96.8**. DistilBERT can be trained to improve its score on this task – a process called **fine-tuning** which updates BERT’s weights to make it achieve a better performance in this sentence classification task (which we can call the downstream task). The fine-tuned DistilBERT turns out to achieve an accuracy score of **90.7**. The full size BERT model achieves **94.9**.\n","\n","\n","\n","And that’s it! That’s a good first contact with BERT. The next step would be to head over to the documentation and try your hand at [fine-tuning](https://huggingface.co/transformers/examples.html#glue). You can also go back and switch from distilBERT to BERT and see how that works."]},{"cell_type":"code","metadata":{"colab_type":"code","id":"EJQuqV6cnWQu","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"u4xpfJB5Nrjd","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"4EAo6_7_Ey3G","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]}]}