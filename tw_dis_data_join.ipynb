{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.util import ngrams\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from collections import defaultdict\n",
    "from collections import  Counter\n",
    "plt.style.use('ggplot')\n",
    "stop=set(stopwords.words('english'))\n",
    "import re\n",
    "from nltk.tokenize import word_tokenize\n",
    "import string\n",
    "from sklearn.model_selection import train_test_split\n",
    "from bs4 import BeautifulSoup\n",
    "import emoji\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3263, 4)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweet= pd.read_csv('train.csv')\n",
    "test_tw=pd.read_csv('test.csv')\n",
    "test_tw.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#удаляю дубликаты из train.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#поиск дубликатов с разными targets. Раскомментировать, если нужно пересчитать \n",
    "#tweet[\"tt\"] = tweet[\"target\"].astype(\"str\").str.cat(tweet[\"text\"], sep = \" \", na_rep=' ')\n",
    "#mask = tweet[\"text\"].duplicated(keep = False) \n",
    "#mask2 = tweet[mask][\"tt\"].duplicated(keep = False)\n",
    "#print(df[mask & mask1 & -mask2][60:80])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "remove_ids = [353, 390, 872, 875, 1771, 1773, 4051,4076, 4080, 4092, 4499, 4507, 4918, 4935, \n",
    "6012, 6017, 6217, 6219, 6220, 6223, 6366, 6382, 6775, 6788, 7362, 7386, 8018, 8044, 7602, 7626,\n",
    "8296, 8297, 9499, 9501, 796, 822]\n",
    "#print(df[df[\"id\"] == remove_ids[0]])\n",
    "#print(df[df[\"id\"] == remove_ids[0]].iloc[0,0])\n",
    "print(len(remove_ids))\n",
    "\n",
    "for i in remove_ids:\n",
    "    rm = tweet[tweet[\"id\"] == i].index[0]\n",
    "    #print(df.iloc[rm])\n",
    "    tweet = tweet.drop([rm])\n",
    "    print(i, rm, tweet.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7613, 5)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df=pd.concat([tweet,test_tw], sort=\"False\")\n",
    "df.shape\n",
    "tweet.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = re.compile(r'https?://\\S+|www\\.\\S+')\n",
    "\n",
    "def get_URL(text):\n",
    "    u = re.findall(url, text)\n",
    "    return u if u else ''  \n",
    "\n",
    "def remove_URL(text):\n",
    "    return re.sub(url, r\"\", text)\n",
    "\n",
    "def remove_HTML(text):\n",
    "    soup = BeautifulSoup(text)\n",
    "    return soup.get_text()\n",
    "\n",
    "# Reference : https://gist.github.com/slowkow/7a7f61f495e3dbb7e3d767f97bd7304b\n",
    "#emoji_pattern = re.compile(\"[\"\n",
    "#                           u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "#                           u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "#                           u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "#                           u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "#                           u\"\\U00002702-\\U000027B0\"\n",
    "#                           u\"\\U000024C2-\\U0001F251\"\n",
    "#                           \"]+\", flags=re.UNICODE)\n",
    "emoji_pattern = re.compile(\"[\"u\"\\U0001F600-\\U0001F64F\"\"]+\", flags=re.UNICODE)\n",
    "\n",
    "\n",
    "def get_emoji(text):\n",
    "    u = re.findall(emoji_pattern, text)\n",
    "    return u\n",
    "\n",
    "def emoji2text(text):\n",
    "    em = re.search(emoji_pattern, text)\n",
    "    if not(em is None):\n",
    "        u = emoji.demojize(text)\n",
    "        print(u)\n",
    "        return u\n",
    "    else:\n",
    "        return text\n",
    "\n",
    "def emoji2text2(text):\n",
    "    return emoji.demojize(text)\n",
    "\n",
    "def remove_emoji(text):\n",
    "    return emoji_pattern.sub(r'', text)\n",
    "\n",
    "def get_tags(x):\n",
    "    return {str(tag.strip(\"#\")) for tag in x.split() if tag.startswith(\"#\")}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df['text_original'] = df[\"text\"]\n",
    "#df['text'] = df['text'].apply(lambda x: remove_HTML(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#предположим, что url пригодится, как отдельный признак. От url можно title раздобыть\n",
    "df['url'] = df['text'].apply(lambda x: get_URL(x))\n",
    "#df['text'] = df['text'].apply(lambda x: remove_URL(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#предположим, что emoji пригодятся, как категориальный признак\n",
    "#df[\"emoji\"] = df['text'].apply(lambda x: get_emoji(x))\n",
    "#df['text'] = df['text'].apply(lambda x: remove_emoji(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#кажется смайликов там нет. \n",
    "#df['text'] = df['text'].apply(lambda x: emoji2text2(x))\n",
    "#f = open('data_deemoji.csv', 'w')\n",
    "#df.to_csv(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#вытаскиваем хештеги\n",
    "#df[\"tags\"] =  df['text'].apply(lambda x: get_tags(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#убираем пунктуацию - в этой версии оставим\n",
    "#df['text'] = df['text'].apply(lambda x: x.translate(str.maketrans('','',string.punctuation)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# клеим текст, location, keyword\n",
    "df[\"text\"] = df[\"text\"].str.cat(df[\"keyword\"], sep = \" \", na_rep=' ').str.cat(df[\"location\"], sep = \" \", na_rep=' ')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>target</th>\n",
       "      <th>text</th>\n",
       "      <th>url</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>245</td>\n",
       "      <td>348</td>\n",
       "      <td>annihilation</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Evildead - Annihilation of Civilization http:/...</td>\n",
       "      <td>[http://t.co/sPfkE5Kqu4]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>246</td>\n",
       "      <td>349</td>\n",
       "      <td>annihilation</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>U.S National Park Services Tonto National Fore...</td>\n",
       "      <td>[http://t.co/6LoJOoROuk]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>247</td>\n",
       "      <td>352</td>\n",
       "      <td>annihilation</td>\n",
       "      <td>Wild Wild Web</td>\n",
       "      <td>1.0</td>\n",
       "      <td>annihilating quarterstaff of annihilation anni...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>248</td>\n",
       "      <td>353</td>\n",
       "      <td>annihilation</td>\n",
       "      <td>Subconscious LA</td>\n",
       "      <td>0.0</td>\n",
       "      <td>World Annihilation vs Self Transformation http...</td>\n",
       "      <td>[http://t.co/pyehwodWun, http://t.co/pB2N77nSKz]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>249</td>\n",
       "      <td>354</td>\n",
       "      <td>annihilation</td>\n",
       "      <td>Spain</td>\n",
       "      <td>1.0</td>\n",
       "      <td>:StarMade: :Stardate 3: :Planetary Annihilatio...</td>\n",
       "      <td>[http://t.co/I2hHvIUmTm]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      id       keyword         location  target  \\\n",
       "245  348  annihilation              NaN     0.0   \n",
       "246  349  annihilation              NaN     0.0   \n",
       "247  352  annihilation    Wild Wild Web     1.0   \n",
       "248  353  annihilation  Subconscious LA     0.0   \n",
       "249  354  annihilation            Spain     1.0   \n",
       "\n",
       "                                                  text  \\\n",
       "245  Evildead - Annihilation of Civilization http:/...   \n",
       "246  U.S National Park Services Tonto National Fore...   \n",
       "247  annihilating quarterstaff of annihilation anni...   \n",
       "248  World Annihilation vs Self Transformation http...   \n",
       "249  :StarMade: :Stardate 3: :Planetary Annihilatio...   \n",
       "\n",
       "                                                  url  \n",
       "245                          [http://t.co/sPfkE5Kqu4]  \n",
       "246                          [http://t.co/6LoJOoROuk]  \n",
       "247                                                    \n",
       "248  [http://t.co/pyehwodWun, http://t.co/pB2N77nSKz]  \n",
       "249                          [http://t.co/I2hHvIUmTm]  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[245:250]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "from spellchecker import SpellChecker\n",
    "spell = SpellChecker()\n",
    "list = ['wildfire', 'wildfires', 'rockyfire', 'ebay', 'coworker', 'youtube', 'hailstorm', 'instagram', 'ebola', 'iphone']\n",
    "for l in list:\n",
    "    spell.word_frequency.add(l)\n",
    "\n",
    "def correct_spellings(text):\n",
    "    corrected_text = []\n",
    "    misspelled_words = spell.unknown(text.split())\n",
    "    if len(misspelled_words) > 0:\n",
    "        print(misspelled_words)\n",
    "        for word in misspelled_words:\n",
    "            text = re.sub(word, spell.correction(word), text)\n",
    "            #print(text)\n",
    "    return text\n",
    "\n",
    "#print(\"resp:\", correct_spellings(\"corect my nam, ples\"))\n",
    "        \n",
    "df['text']=df['text'].apply(lambda x : correct_spellings(x))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10876, 6)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       \"Our Deeds are the Reason of this #earthquake ...\n",
       "1            \"Forest fire near La Ronge Sask. Canada    \"\n",
       "2       \"All residents asked to 'shelter in place' are...\n",
       "3       \"13,000 people receive #wildfires evacuation o...\n",
       "4       \"Just got sent this photo from Ruby #Alaska as...\n",
       "                              ...                        \n",
       "3258    \"EARTHQUAKE SAFETY LOS ANGELES ÛÒ SAFETY FAST...\n",
       "3259    \"Storm in RI worse than last hurricane. My cit...\n",
       "3260    \"Green Line derailment in Chicago http://t.co/...\n",
       "3261    \"MEG issues Hazardous Weather Outlook (HWO) ht...\n",
       "3262    \"#CityofCalgary has activated its Municipal Em...\n",
       "Name: text, Length: 10876, dtype: object"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#закавычивание строк с запятыми внутри\"\n",
    "df[\"text\"].apply(lambda x: x if re.findall(\",\",x) is None else \"\".join(['\"',x,'\"']))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10876, 6)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"text\"].loc[10813]= \"@stighefootball Begovic has been garbage. He got wrecked by a Red Bull reserve team and everyone else this preseason wrecked Brussels, Belgium\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3263, 7)\n",
      "(10876, 7)\n"
     ]
    }
   ],
   "source": [
    "f = open('data_proc3.csv', 'w')\n",
    "sep = \",\"\n",
    "df.to_csv(f, sep=sep)\n",
    "f.close\n",
    "test = pd.read_csv('data_proc3.csv', sep=sep)\n",
    "print(test[test[\"target\"].isna()].shape)\n",
    "print(test.shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
